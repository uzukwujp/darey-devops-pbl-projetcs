# Logging concepts and best practices

## Why are logs important?

* Logs serve as a vital tool for:
* Troubleshooting: Identify and diagnose issues, bugs, and errors.
* Security monitoring: Detect suspicious activity and potential threats.
* Performance analysis: Understand how your systems are performing and optimize resource usage.
* Auditing and compliance: Maintain compliance with regulations and track user activity.

## Best Practices
* Standardize your logging format: Use a structured format like JSON or XML for easier parsing and analysis.
* Choose the right log storage: Select a reliable and scalable storage solution based on your volume and needs.
* Centralize your logs: Aggregate logs from various sources for easier search and analysis.
* Enable log rotation: Set up automatic log rotation to avoid disk space exhaustion.
* Secure your logs: Implement access control and encryption to protect sensitive information.
* Monitor your logs: Set up alerts for critical events and errors.
*Analyze your logs: Extract meaningful insights and trends from your log data.

We are imitating centralized logging concept.

All the total micro-services generated will be collected and store in one single central place.

## Centralized logging architecture

![elastic-stack](../images/elastic-stack.png)

To implement centralized logging, we need to deploy **elastic stack**.
To collect the logs that are generated by each of the pod of kubernetes cluster. We use application called **fluentd**.
**Fluentd** is a log collector.

Instructions on how fluentd should work, is store in its configuration file called **fluentd.conf**. To change fluentd setting, its the configuration file that we keep on modifying. 
You keep the **fluentd.conf** as a resources of type configmap and give it to fluentd.

Fluentd collect logs on a single worker nodes. If you have multiple worker nodes, make sure , you have fluentd running in all the worker nodes. To make sure that each wokernodes has flunetd running on it, you should deploy it with daemon set controller.
The logs collected from each woker nodes by fluentd are push in a database called **Elasticsearch**.

**Elasticsearch** is a database where the logs will be store.
**Kibana** is the dashboad . The collected data will be collected from *elasticsearch* and shown in a beautiful graphical format.
*Kibana* is just a log search and log agregator.
If there is an error in our pod, we go to *kibana* dashboard and search if any error are reported.

`Elasticsearch, kibana and flueentd` are deploy into kubernetes as a pod and connected via clusterIP.
Fluentd go inside a pod and access it with the help of service account.