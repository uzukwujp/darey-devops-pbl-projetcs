# Deploy  and configure ELK stack

```
mkdir elastic stack
cd elastic stack

```
## Create fluentd-config file

Create a fluentd manifestfile [fluentd.yaml](../project62/fluentd.yaml).

This is created in kube-system because its a management related service.

Fluentd deployment kind is **DeamonSet**.

```
vi fluentd.yaml
kubectl apply -f fluentd.yaml
kubectl get configmap -n kube-system
```

## Deploy elastic-stack

Create  elastic-stack manifestfile [elastic-stack](../project62/elastic-stack.yaml)

This file will deploy both Kibana and Elasticsearch.

```
vi elastic-stack.yaml

```
**ServiceAccount** - fluentd-es service account will be created
**ClusteRole** is use to create a role with a kind. This give permission.

Whatever service using the role will have permision to "list,get,watch" on `pod and namespaces`.

**ClusterRoleBinding** - who to use the role i.e fluentd-es service account should use the role called **fluentd-es.

If you use **DeamonSet** controller kind of deployment, you will not need to state replicas set controller count. If there are 4 workernode, there will be 4 fluentd pod. This is manage/created automatically when you use daemonset deployment type.
The Elasticsearch image is pull from google registry.
The kibana image is pull from docker registry.


Elastic-stack are manageged services and they are not customized as they are generic applications.

```
kubectl apply -f elastic-stack.yaml
kubectl get pods -n kube-system
kubectl get nodes
```
Fluentd will be created for each nodes within your cluster.

To see all the service in the kube-system namespace.
```
kubectl get service -n kube-system
```

After deployment, use kubectl get service kibana to find the assigned NodePort. This will be a random port number within a specific range based on your cluster configuration.
Access Kibana from any device within your cluster network using 

Get the NodePort assigned to the Kibana service:

```
kubectl get service kibana-logging -n kube-system
```
Forward the NodePort to your local machine's port 5601:

```
kubectl port-forward service/kibana-logging -n kube-system 5601:5601
```
Access Kibana at:

```
http://localhost:5601 in your browser
```
![kibana-dashboard](../project62/images/kibana-dashboard.png)

**Note**

* Port forwarding exposes Kibana externally, so exercise caution and implement security measures if you choose this method.
* For production environments, consider using an Ingress controller with appropriate security restrictions.
* Avoid using kubectl proxy for production due to potential vulnerabilities.

### Additional options for production environments:

* Internal load balancer: This offers more control and security than NodePort services.
* Move Kibana to a dedicated namespace: This eliminates the need for a NetworkPolicy to access Kibana from other namespaces.


# Step to create index pattern (setting up kibana)

This settings will allow kibana to get data from elasticsearch.

Management - Advanced Setting

Table is to database why index is to elasticsearch.

1. Define index pattern
    
   -  copy logstah-2024.02.17 to the space given for the index name.

   -  click on next step

   ![index-pattern](../project62/images/define-index-pattern.png)

2. configure settings
    - Time filter field name - @Timestamp
    - create index pattern

![configure-setting](../project62/images/configure-setting.png)

## created index pattern

Click on discovery

![logstash](../project62/images/logstash.png)

![logs-pods](../project62/images/pods-logs.png)

You can see how many logs are generated by your pods and what time.

you can search for pods logs that have word call **error**

you can see time error occurs, which contaer is reporting the problem and what type of error.
You will see the error reported.

You can check log of a specific pod, if you know the pod that has problem.
You can see logs at pods, namespace level etc